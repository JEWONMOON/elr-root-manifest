"""
Practical Scientific Reasoning Core v2.0
ì‹¤ì œë¡œ ì‘ë™í•˜ëŠ” ì‹¤ìš©ì  ì¶”ë¡  ì‹œìŠ¤í…œ

ì² í•™:
- ë³µì¡ì„± < íˆ¬ëª…ì„±
- ì™„ë²½í•¨ < ìœ ìš©í•¨  
- ì´ë¡  < ì‹¤ì „
- í—ˆì„¸ < ì •ì§

"ì¶”ë¡ í•˜ëŠ” ì²™"ì´ ì•„ë‹Œ "ì‹¤ì œ ì¶”ë¡ "ì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.
"""

import numpy as np
from typing import Dict, List, Any, Optional, Tuple, Set, Union
from dataclasses import dataclass, field
from datetime import datetime
from collections import defaultdict, deque
import json
import hashlib


# ======================== 1. SIMPLE BAYESIAN REASONER ========================
class SimpleBayesianReasoner:
    """ì‹¤ì œë¡œ ì‘ë™í•˜ëŠ” ë² ì´ì§€ì•ˆ ì¶”ë¡  ì—”ì§„"""
    
    def __init__(self):
        self.beliefs: Dict[str, float] = {}
        self.evidence_history: List[Dict[str, Any]] = []
        self.prior_defaults = {
            "very_likely": 0.8,
            "likely": 0.7,
            "neutral": 0.5,
            "unlikely": 0.3,
            "very_unlikely": 0.2
        }
    
    def set_prior(self, hypothesis: str, probability: float = 0.5, 
                  confidence: str = "neutral") -> None:
        """ì‚¬ì „ í™•ë¥  ì„¤ì •"""
        if confidence in self.prior_defaults:
            probability = self.prior_defaults[confidence]
        
        self.beliefs[hypothesis] = max(0.001, min(0.999, probability))
    
    def update_belief(self, hypothesis: str, evidence: str, 
                     likelihood_if_true: float, 
                     likelihood_if_false: float) -> Dict[str, float]:
        """ë² ì´ì¦ˆ ì •ë¦¬ë¥¼ ì‚¬ìš©í•œ ì‹ ë… ì—…ë°ì´íŠ¸"""
        
        # ì‚¬ì „ í™•ë¥ 
        prior = self.beliefs.get(hypothesis, 0.5)
        
        # ë² ì´ì¦ˆ ì •ë¦¬: P(H|E) = P(E|H)P(H) / P(E)
        # P(E) = P(E|H)P(H) + P(E|~H)P(~H)
        p_evidence = (likelihood_if_true * prior + 
                     likelihood_if_false * (1 - prior))
        
        if p_evidence == 0:
            posterior = prior  # ì¦ê±°ê°€ ë¶ˆê°€ëŠ¥í•œ ê²½ìš°
        else:
            posterior = (likelihood_if_true * prior) / p_evidence
        
        # ì—…ë°ì´íŠ¸
        self.beliefs[hypothesis] = posterior
        
        # ê¸°ë¡
        update_record = {
            "hypothesis": hypothesis,
            "evidence": evidence,
            "prior": prior,
            "posterior": posterior,
            "change": posterior - prior,
            "timestamp": datetime.now().isoformat()
        }
        self.evidence_history.append(update_record)
        
        return update_record
    
    def get_most_likely_hypothesis(self) -> Tuple[str, float]:
        """ê°€ì¥ ê°€ëŠ¥ì„± ë†’ì€ ê°€ì„¤ ë°˜í™˜"""
        if not self.beliefs:
            return None, 0.0
        
        return max(self.beliefs.items(), key=lambda x: x[1])
    
    def compare_hypotheses(self, h1: str, h2: str) -> Dict[str, Any]:
        """ë‘ ê°€ì„¤ ë¹„êµ"""
        p1 = self.beliefs.get(h1, 0.5)
        p2 = self.beliefs.get(h2, 0.5)
        
        # ë² ì´ì¦ˆ íŒ©í„° ê³„ì‚°
        if p2 > 0 and (1-p1) > 0:
            bayes_factor = (p1 / (1-p1)) / (p2 / (1-p2))
        else:
            bayes_factor = float('inf') if p1 > p2 else 0
        
        return {
            f"P({h1})": p1,
            f"P({h2})": p2,
            "bayes_factor": bayes_factor,
            "interpretation": self._interpret_bayes_factor(bayes_factor),
            "preferred": h1 if p1 > p2 else h2
        }
    
    def _interpret_bayes_factor(self, bf: float) -> str:
        """ë² ì´ì¦ˆ íŒ©í„° í•´ì„"""
        if bf > 100:
            return "ê²°ì •ì  ì¦ê±°"
        elif bf > 10:
            return "ê°•í•œ ì¦ê±°"
        elif bf > 3:
            return "ì¤‘ê°„ ì¦ê±°"
        elif bf > 1:
            return "ì•½í•œ ì¦ê±°"
        elif bf == 1:
            return "ì¤‘ë¦½"
        else:
            return "ë°˜ëŒ€ ì¦ê±°"


# ======================== 2. PRACTICAL LOGIC ENGINE ========================
class PracticalLogicEngine:
    """ì‹¤ìš©ì  ë…¼ë¦¬ ì¶”ë¡  ì—”ì§„"""
    
    def __init__(self):
        self.facts: Set[str] = set()
        self.rules: List[Dict[str, Any]] = []
        self.inference_history: List[Dict[str, Any]] = []
    
    def add_fact(self, fact: str) -> None:
        """ì‚¬ì‹¤ ì¶”ê°€"""
        self.facts.add(fact)
    
    def add_rule(self, name: str, conditions: List[str], 
                 conclusion: str, confidence: float = 1.0) -> None:
        """ì¶”ë¡  ê·œì¹™ ì¶”ê°€"""
        self.rules.append({
            "name": name,
            "conditions": conditions,
            "conclusion": conclusion,
            "confidence": confidence
        })
    
    def forward_inference(self, max_iterations: int = 10) -> List[str]:
        """ì „ë°© ì¶”ë¡  ì‹¤í–‰"""
        new_facts_total = []
        
        for iteration in range(max_iterations):
            new_facts = []
            
            for rule in self.rules:
                # ëª¨ë“  ì¡°ê±´ì´ ë§Œì¡±ë˜ëŠ”ì§€ í™•ì¸
                if all(cond in self.facts for cond in rule["conditions"]):
                    if rule["conclusion"] not in self.facts:
                        new_facts.append(rule["conclusion"])
                        self.facts.add(rule["conclusion"])
                        
                        # ì¶”ë¡  ê¸°ë¡
                        self.inference_history.append({
                            "rule": rule["name"],
                            "conditions": rule["conditions"],
                            "conclusion": rule["conclusion"],
                            "iteration": iteration
                        })
            
            if not new_facts:
                break  # ë” ì´ìƒ ìƒˆë¡œìš´ ì‚¬ì‹¤ì´ ì—†ìœ¼ë©´ ì¢…ë£Œ
                
            new_facts_total.extend(new_facts)
        
        return new_facts_total
    
    def check_consistency(self) -> Dict[str, Any]:
        """ë…¼ë¦¬ì  ì¼ê´€ì„± ê²€ì‚¬"""
        contradictions = []
        
        # ì§ì ‘ì ì¸ ëª¨ìˆœ ì°¾ê¸° (Aì™€ ~A)
        for fact in self.facts:
            if fact.startswith("~"):
                positive = fact[1:]
                if positive in self.facts:
                    contradictions.append((positive, fact))
            else:
                negative = f"~{fact}"
                if negative in self.facts:
                    contradictions.append((fact, negative))
        
        return {
            "is_consistent": len(contradictions) == 0,
            "contradictions": contradictions,
            "fact_count": len(self.facts),
            "rule_count": len(self.rules)
        }
    
    def explain_inference(self, target_fact: str) -> List[Dict[str, Any]]:
        """íŠ¹ì • ì‚¬ì‹¤ì— ëŒ€í•œ ì¶”ë¡  ê²½ë¡œ ì„¤ëª…"""
        explanation = []
        
        for record in self.inference_history:
            if record["conclusion"] == target_fact:
                explanation.append({
                    "step": len(explanation) + 1,
                    "rule_used": record["rule"],
                    "required_facts": record["conditions"],
                    "derived_fact": record["conclusion"]
                })
        
        return explanation


# ======================== 3. UNCERTAINTY HANDLER ========================
class UncertaintyHandler:
    """ë¶ˆí™•ì‹¤ì„± ì²˜ë¦¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.confidence_threshold = 0.7
        self.uncertainty_sources = {
            "incomplete_data": 0.3,
            "measurement_error": 0.1,
            "model_uncertainty": 0.2,
            "unknown_factors": 0.4
        }
    
    def calculate_confidence(self, 
                           evidence_quality: float,
                           model_accuracy: float,
                           consistency_score: float) -> Dict[str, float]:
        """ì¢…í•©ì  ì‹ ë¢°ë„ ê³„ì‚°"""
        
        # ê°€ì¤‘ í‰ê· 
        weights = {"evidence": 0.4, "model": 0.3, "consistency": 0.3}
        
        weighted_score = (
            evidence_quality * weights["evidence"] +
            model_accuracy * weights["model"] +
            consistency_score * weights["consistency"]
        )
        
        # ë¶ˆí™•ì‹¤ì„± ìš”ì¸ ê³ ë ¤
        uncertainty_penalty = sum(
            self.uncertainty_sources.values()
        ) / len(self.uncertainty_sources)
        
        final_confidence = weighted_score * (1 - uncertainty_penalty)
        
        return {
            "raw_confidence": weighted_score,
            "uncertainty_penalty": uncertainty_penalty,
            "final_confidence": final_confidence,
            "is_reliable": final_confidence >= self.confidence_threshold,
            "interpretation": self._interpret_confidence(final_confidence)
        }
    
    def propagate_uncertainty(self, 
                            input_uncertainties: List[float],
                            operation: str = "multiply") -> float:
        """ë¶ˆí™•ì‹¤ì„± ì „íŒŒ ê³„ì‚°"""
        
        if operation == "multiply":
            # ê³±ì…ˆì˜ ê²½ìš°: ìƒëŒ€ ë¶ˆí™•ì‹¤ì„±ì˜ ì œê³±í•©ì˜ ì œê³±ê·¼
            if len(input_uncertainties) == 0:
                return 0
            return np.sqrt(sum(u**2 for u in input_uncertainties))
        
        elif operation == "add":
            # ë§ì…ˆì˜ ê²½ìš°: ì ˆëŒ€ ë¶ˆí™•ì‹¤ì„±ì˜ ì œê³±í•©ì˜ ì œê³±ê·¼
            return np.sqrt(sum(u**2 for u in input_uncertainties))
        
        else:
            # ê¸°ë³¸: ìµœëŒ€ ë¶ˆí™•ì‹¤ì„±
            return max(input_uncertainties) if input_uncertainties else 0
    
    def _interpret_confidence(self, confidence: float) -> str:
        """ì‹ ë¢°ë„ ìˆ˜ì¤€ í•´ì„"""
        if confidence >= 0.9:
            return "ë§¤ìš° ë†’ìŒ"
        elif confidence >= 0.7:
            return "ë†’ìŒ"
        elif confidence >= 0.5:
            return "ì¤‘ê°„"
        elif confidence >= 0.3:
            return "ë‚®ìŒ"
        else:
            return "ë§¤ìš° ë‚®ìŒ"


# ======================== 4. CAUSAL REASONER ========================
class CausalReasoner:
    """ì¸ê³¼ê´€ê³„ ì¶”ë¡  ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.causal_graph = {}  # cause -> [effects]
        self.observations = []
        self.confounders = set()
    
    def add_causal_link(self, cause: str, effect: str, 
                       strength: float = 1.0) -> None:
        """ì¸ê³¼ê´€ê³„ ì¶”ê°€"""
        if cause not in self.causal_graph:
            self.causal_graph[cause] = []
        
        self.causal_graph[cause].append({
            "effect": effect,
            "strength": strength
        })
    
    def add_confounder(self, variable: str) -> None:
        """êµë€ë³€ìˆ˜ ì¶”ê°€"""
        self.confounders.add(variable)
    
    def observe(self, variable: str, value: Any, 
                context: Dict[str, Any] = None) -> None:
        """ê´€ì°° ê¸°ë¡"""
        self.observations.append({
            "variable": variable,
            "value": value,
            "context": context or {},
            "timestamp": datetime.now().isoformat()
        })
    
    def infer_causation(self, cause: str, effect: str) -> Dict[str, Any]:
        """ì¸ê³¼ê´€ê³„ ì¶”ë¡ """
        
        # ì§ì ‘ ì¸ê³¼ê´€ê³„ í™•ì¸
        direct_link = None
        if cause in self.causal_graph:
            for link in self.causal_graph[cause]:
                if link["effect"] == effect:
                    direct_link = link
                    break
        
        # ê°„ì ‘ ê²½ë¡œ ì°¾ê¸°
        indirect_paths = self._find_causal_paths(cause, effect)
        
        # ê´€ì°° ë°ì´í„°ì—ì„œ ìƒê´€ê´€ê³„ í™•ì¸
        correlation = self._calculate_correlation(cause, effect)
        
        # êµë€ë³€ìˆ˜ ì˜í–¥ í‰ê°€
        confounder_risk = self._assess_confounder_risk(cause, effect)
        
        return {
            "direct_causation": direct_link is not None,
            "direct_strength": direct_link["strength"] if direct_link else 0,
            "indirect_paths": len(indirect_paths),
            "correlation": correlation,
            "confounder_risk": confounder_risk,
            "causal_confidence": self._calculate_causal_confidence(
                direct_link, indirect_paths, correlation, confounder_risk
            )
        }
    
    def _find_causal_paths(self, start: str, end: str, 
                          path: List[str] = None) -> List[List[str]]:
        """ì¸ê³¼ ê²½ë¡œ ì°¾ê¸° (DFS)"""
        if path is None:
            path = [start]
        
        if start == end:
            return [path]
        
        if start not in self.causal_graph:
            return []
        
        paths = []
        for link in self.causal_graph[start]:
            next_node = link["effect"]
            if next_node not in path:  # ìˆœí™˜ ë°©ì§€
                new_path = path + [next_node]
                paths.extend(
                    self._find_causal_paths(next_node, end, new_path)
                )
        
        return paths
    
    def _calculate_correlation(self, var1: str, var2: str) -> float:
        """ê´€ì°° ë°ì´í„°ì—ì„œ ìƒê´€ê´€ê³„ ê³„ì‚°"""
        # ê°„ë‹¨í•œ êµ¬í˜„ - ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ í†µê³„ í•„ìš”
        var1_obs = [o["value"] for o in self.observations 
                   if o["variable"] == var1]
        var2_obs = [o["value"] for o in self.observations 
                   if o["variable"] == var2]
        
        if len(var1_obs) < 2 or len(var2_obs) < 2:
            return 0.0
        
        # ë™ì‹œ ë°œìƒ ë¹„ìœ¨ë¡œ ê°„ë‹¨íˆ ì¶”ì •
        co_occurrence = 0
        for obs in self.observations:
            context = obs.get("context", {})
            if var1 in context and var2 in context:
                co_occurrence += 1
        
        return co_occurrence / max(len(var1_obs), len(var2_obs))
    
    def _assess_confounder_risk(self, cause: str, effect: str) -> float:
        """êµë€ë³€ìˆ˜ ìœ„í—˜ë„ í‰ê°€"""
        risk = 0.0
        
        # ê° êµë€ë³€ìˆ˜ê°€ ì›ì¸ê³¼ ê²°ê³¼ ëª¨ë‘ì— ì˜í–¥ì„ ì£¼ëŠ”ì§€ í™•ì¸
        for confounder in self.confounders:
            affects_cause = confounder in self.causal_graph and \
                          any(link["effect"] == cause 
                              for link in self.causal_graph[confounder])
            affects_effect = confounder in self.causal_graph and \
                           any(link["effect"] == effect 
                               for link in self.causal_graph[confounder])
            
            if affects_cause and affects_effect:
                risk += 0.3  # ê° êµë€ë³€ìˆ˜ë‹¹ ìœ„í—˜ë„ ì¦ê°€
        
        return min(risk, 1.0)
    
    def _calculate_causal_confidence(self, direct_link, indirect_paths, 
                                   correlation, confounder_risk) -> float:
        """ì¢…í•©ì  ì¸ê³¼ê´€ê³„ ì‹ ë¢°ë„"""
        confidence = 0.0
        
        # ì§ì ‘ ì¸ê³¼ê´€ê³„ê°€ ìˆìœ¼ë©´ ê¸°ë³¸ ì‹ ë¢°ë„
        if direct_link:
            confidence += direct_link["strength"] * 0.5
        
        # ê°„ì ‘ ê²½ë¡œë„ ê³ ë ¤
        if indirect_paths:
            confidence += min(len(indirect_paths) * 0.1, 0.3)
        
        # ìƒê´€ê´€ê³„ë„ ê³ ë ¤
        confidence += correlation * 0.2
        
        # êµë€ë³€ìˆ˜ ìœ„í—˜ë„ë§Œí¼ ê°ì†Œ
        confidence *= (1 - confounder_risk * 0.5)
        
        return min(confidence, 1.0)


# ======================== 5. BIAS DETECTOR ========================
class BiasDetector:
    """ì¸ì§€ í¸í–¥ ê°ì§€ê¸°"""
    
    def __init__(self):
        self.detected_biases = []
        self.bias_patterns = {
            "confirmation": self._check_confirmation_bias,
            "anchoring": self._check_anchoring_bias,
            "availability": self._check_availability_bias,
            "complexity": self._check_complexity_bias
        }
    
    def analyze_reasoning(self, reasoning_trace: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ì¶”ë¡  ê³¼ì •ì—ì„œ í¸í–¥ ê°ì§€"""
        
        detected = {}
        
        for bias_name, check_func in self.bias_patterns.items():
            result = check_func(reasoning_trace)
            if result["detected"]:
                detected[bias_name] = result
        
        # ì „ì²´ í¸í–¥ ì ìˆ˜
        bias_score = len(detected) / len(self.bias_patterns)
        
        return {
            "detected_biases": detected,
            "bias_score": bias_score,
            "is_biased": bias_score > 0.3,
            "recommendations": self._generate_debiasing_recommendations(detected)
        }
    
    def _check_confirmation_bias(self, trace: List[Dict[str, Any]]) -> Dict[str, Any]:
        """í™•ì¦í¸í–¥ ê²€ì‚¬"""
        
        supporting_evidence = 0
        contradicting_evidence = 0
        
        for step in trace:
            if step.get("type") == "evidence":
                if step.get("supports_hypothesis"):
                    supporting_evidence += 1
                else:
                    contradicting_evidence += 1
        
        total = supporting_evidence + contradicting_evidence
        if total == 0:
            return {"detected": False}
        
        support_ratio = supporting_evidence / total
        
        # ì§€ì§€ ì¦ê±°ê°€ 80% ì´ìƒì´ë©´ í™•ì¦í¸í–¥ ì˜ì‹¬
        if support_ratio > 0.8:
            return {
                "detected": True,
                "severity": "high" if support_ratio > 0.9 else "medium",
                "evidence": f"ì§€ì§€ ì¦ê±° {support_ratio:.0%}",
                "recommendation": "ë°˜ëŒ€ ì¦ê±°ë¥¼ ì ê·¹ì ìœ¼ë¡œ ì°¾ì•„ë³´ì„¸ìš”"
            }
        
        return {"detected": False}
    
    def _check_anchoring_bias(self, trace: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ê¸°ì¤€ì  í¸í–¥ ê²€ì‚¬"""
        
        initial_estimates = []
        final_estimates = []
        
        for i, step in enumerate(trace):
            if step.get("type") == "estimate":
                if i < len(trace) / 3:  # ì´ˆê¸° 1/3
                    initial_estimates.append(step.get("value", 0))
                elif i > 2 * len(trace) / 3:  # í›„ê¸° 1/3
                    final_estimates.append(step.get("value", 0))
        
        if not initial_estimates or not final_estimates:
            return {"detected": False}
        
        # ì´ˆê¸° ì¶”ì •ì¹˜ ì£¼ë³€ì— ìµœì¢… ì¶”ì •ì¹˜ê°€ ëª¨ì—¬ìˆëŠ”ì§€ í™•ì¸
        initial_mean = np.mean(initial_estimates)
        final_std = np.std(final_estimates)
        
        if final_std < initial_mean * 0.2:  # ë³€ë™ì„±ì´ ì‘ìœ¼ë©´ ê¸°ì¤€ì  í¸í–¥ ì˜ì‹¬
            return {
                "detected": True,
                "severity": "medium",
                "evidence": f"ìµœì¢… ì¶”ì •ì¹˜ í‘œì¤€í¸ì°¨: {final_std:.2f}",
                "recommendation": "ì´ˆê¸° ê°€ì •ì„ ì˜ì‹¬í•˜ê³  ë‹¤ì‹œ ìƒê°í•´ë³´ì„¸ìš”"
            }
        
        return {"detected": False}
    
    def _check_availability_bias(self, trace: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ê°€ìš©ì„± í¸í–¥ ê²€ì‚¬"""
        
        recent_examples = 0
        old_examples = 0
        
        for step in trace:
            if step.get("type") == "example":
                age = step.get("age_days", 0)
                if age < 7:  # ìµœê·¼ 1ì£¼ì¼
                    recent_examples += 1
                else:
                    old_examples += 1
        
        if recent_examples + old_examples == 0:
            return {"detected": False}
        
        recency_ratio = recent_examples / (recent_examples + old_examples)
        
        if recency_ratio > 0.7:
            return {
                "detected": True,
                "severity": "medium",
                "evidence": f"ìµœê·¼ ì‚¬ë¡€ ë¹„ìœ¨: {recency_ratio:.0%}",
                "recommendation": "ë” ì˜¤ë˜ë˜ê³  ë‹¤ì–‘í•œ ì‚¬ë¡€ë¥¼ ê³ ë ¤í•˜ì„¸ìš”"
            }
        
        return {"detected": False}
    
    def _check_complexity_bias(self, trace: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ë³µì¡ì„± í¸í–¥ ê²€ì‚¬"""
        
        complexity_scores = []
        preference_scores = []
        
        for step in trace:
            if step.get("type") == "option_evaluation":
                complexity = step.get("complexity", 0)
                preference = step.get("preference", 0)
                
                complexity_scores.append(complexity)
                preference_scores.append(preference)
        
        if len(complexity_scores) < 2:
            return {"detected": False}
        
        # ë³µì¡ì„±ê³¼ ì„ í˜¸ë„ì˜ ìƒê´€ê´€ê³„
        correlation = np.corrcoef(complexity_scores, preference_scores)[0, 1]
        
        if correlation > 0.6:
            return {
                "detected": True,
                "severity": "high" if correlation > 0.8 else "medium",
                "evidence": f"ë³µì¡ì„±-ì„ í˜¸ë„ ìƒê´€: {correlation:.2f}",
                "recommendation": "ë‹¨ìˆœí•œ í•´ê²°ì±…ë„ ì§„ì§€í•˜ê²Œ ê³ ë ¤í•˜ì„¸ìš”"
            }
        
        return {"detected": False}
    
    def _generate_debiasing_recommendations(self, detected_biases: Dict[str, Any]) -> List[str]:
        """í¸í–¥ ì œê±° ê¶Œê³ ì‚¬í•­ ìƒì„±"""
        
        recommendations = []
        
        if "confirmation" in detected_biases:
            recommendations.append("ì ê·¹ì ìœ¼ë¡œ ë°˜ëŒ€ ì¦ê±°ë¥¼ ì°¾ì•„ë³´ì„¸ìš”")
        
        if "anchoring" in detected_biases:
            recommendations.append("ì´ˆê¸° ê°€ì • ì—†ì´ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ë¶„ì„í•´ë³´ì„¸ìš”")
        
        if "availability" in detected_biases:
            recommendations.append("ì²´ê³„ì ì¸ ë°ì´í„° ìˆ˜ì§‘ì„ ê³ ë ¤í•˜ì„¸ìš”")
        
        if "complexity" in detected_biases:
            recommendations.append("Occam's Razor - ê°€ì¥ ë‹¨ìˆœí•œ ì„¤ëª…ì„ ì„ í˜¸í•˜ì„¸ìš”")
        
        if not recommendations:
            recommendations.append("í˜„ì¬ ì¶”ë¡  ê³¼ì •ì€ ê· í˜•ì¡í˜€ ìˆìŠµë‹ˆë‹¤")
        
        return recommendations


# ======================== 6. INTEGRATED REASONER ========================
class PracticalReasoner:
    """ëª¨ë“  êµ¬ì„±ìš”ì†Œë¥¼ í†µí•©í•œ ì‹¤ìš©ì  ì¶”ë¡  ì‹œìŠ¤í…œ"""
    
    def __init__(self, name: str = "PracticalAGI"):
        self.name = name
        self.bayesian = SimpleBayesianReasoner()
        self.logic = PracticalLogicEngine()
        self.uncertainty = UncertaintyHandler()
        self.causal = CausalReasoner()
        self.bias_detector = BiasDetector()
        
        self.reasoning_trace = []
        self.conclusions = []
    
    def reason(self, question: str, context: Dict[str, Any] = None) -> Dict[str, Any]:
        """í†µí•© ì¶”ë¡  ìˆ˜í–‰"""
        
        print(f"\nğŸ§  [{self.name}] ì¶”ë¡  ì‹œì‘: {question}")
        
        # ì¶”ë¡  ì‹œì‘
        self.reasoning_trace = []
        
        # 1. ë¬¸ì œ ë¶„ì„
        problem_type = self._analyze_problem(question, context)
        self._log_step("problem_analysis", problem_type)
        
        # 2. ì ì ˆí•œ ì¶”ë¡  ë°©ë²• ì„ íƒ
        reasoning_methods = self._select_reasoning_methods(problem_type)
        
        results = {}
        
        # 3. ì„ íƒëœ ë°©ë²•ë“¤ë¡œ ì¶”ë¡  ìˆ˜í–‰
        if "bayesian" in reasoning_methods:
            results["bayesian"] = self._apply_bayesian_reasoning(
                problem_type, context
            )
        
        if "logical" in reasoning_methods:
            results["logical"] = self._apply_logical_reasoning(
                problem_type, context
            )
        
        if "causal" in reasoning_methods:
            results["causal"] = self._apply_causal_reasoning(
                problem_type, context
            )
        
        # 4. ê²°ê³¼ í†µí•©
        integrated_conclusion = self._integrate_results(results)
        
        # 5. ë¶ˆí™•ì‹¤ì„± í‰ê°€
        confidence_analysis = self._assess_confidence(
            integrated_conclusion, results
        )
        
        # 6. í¸í–¥ ê²€ì‚¬
        bias_analysis = self.bias_detector.analyze_reasoning(
            self.reasoning_trace
        )
        
        # ìµœì¢… ê²°ê³¼
        final_result = {
            "question": question,
            "answer": integrated_conclusion["answer"],
            "confidence": confidence_analysis["final_confidence"],
            "reasoning_methods": reasoning_methods,
            "detailed_results": results,
            "bias_analysis": bias_analysis,
            "limitations": self._identify_limitations(
                integrated_conclusion, confidence_analysis
            )
        }
        
        self.conclusions.append(final_result)
        
        print(f"âœ… ê²°ë¡ : {final_result['answer']}")
        print(f"ğŸ“Š ì‹ ë¢°ë„: {final_result['confidence']:.2%}")
        
        return final_result
    
    def _analyze_problem(self, question: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """ë¬¸ì œ ìœ í˜• ë¶„ì„"""
        
        problem_type = {
            "type": "unknown",
            "requires_probability": "í™•ë¥ " in question or "ê°€ëŠ¥ì„±" in question,
            "requires_logic": "ë§Œì•½" in question or "ë”°ë¼ì„œ" in question,
            "requires_causation": "ë•Œë¬¸" in question or "ì˜í–¥" in question,
            "has_uncertainty": "ì•„ë§ˆ" in question or "ì¶”ì¸¡" in question
        }
        
        # ë¬¸ì œ ìœ í˜• ê²°ì •
        if problem_type["requires_probability"]:
            problem_type["type"] = "probabilistic"
        elif problem_type["requires_logic"]:
            problem_type["type"] = "logical"
        elif problem_type["requires_causation"]:
            problem_type["type"] = "causal"
        else:
            problem_type["type"] = "general"
        
        return problem_type
    
    def _select_reasoning_methods(self, problem_type: Dict[str, Any]) -> List[str]:
        """ì ì ˆí•œ ì¶”ë¡  ë°©ë²• ì„ íƒ"""
        
        methods = []
        
        if problem_type["type"] == "probabilistic":
            methods.append("bayesian")
        elif problem_type["type"] == "logical":
            methods.append("logical")
        elif problem_type["type"] == "causal":
            methods.append("causal")
            methods.append("bayesian")  # ì¸ê³¼ê´€ê³„ë„ í™•ë¥ ì  ìš”ì†Œ í¬í•¨
        else:
            # ì¼ë°˜ì ì¸ ê²½ìš° ëª¨ë“  ë°©ë²• ì‚¬ìš©
            methods = ["logical", "bayesian"]
        
        return methods
    
    def _apply_bayesian_reasoning(self, problem_type: Dict[str, Any], 
                                context: Dict[str, Any]) -> Dict[str, Any]:
        """ë² ì´ì§€ì•ˆ ì¶”ë¡  ì ìš©"""
        
        # ì»¨í…ìŠ¤íŠ¸ì—ì„œ ê°€ì„¤ê³¼ ì¦ê±° ì¶”ì¶œ
        hypotheses = context.get("hypotheses", ["H1", "H2"])
        evidence = context.get("evidence", [])
        
        # ê° ê°€ì„¤ì— ëŒ€í•´ ë² ì´ì§€ì•ˆ ì—…ë°ì´íŠ¸
        results = {}
        for h in hypotheses:
            self.bayesian.set_prior(h, 0.5)  # ì¤‘ë¦½ì  ì‚¬ì „í™•ë¥ 
            
            for e in evidence:
                update = self.bayesian.update_belief(
                    h, 
                    e.get("description", "evidence"),
                    e.get("likelihood_if_true", 0.8),
                    e.get("likelihood_if_false", 0.3)
                )
                self._log_step("bayesian_update", update)
        
        # ê°€ì¥ ê°€ëŠ¥ì„± ë†’ì€ ê°€ì„¤
        best_hypothesis, probability = self.bayesian.get_most_likely_hypothesis()
        
        return {
            "method": "bayesian",
            "best_hypothesis": best_hypothesis,
            "probability": probability,
            "all_beliefs": dict(self.bayesian.beliefs)
        }
    
    def _apply_logical_reasoning(self, problem_type: Dict[str, Any], 
                                context: Dict[str, Any]) -> Dict[str, Any]:
        """ë…¼ë¦¬ì  ì¶”ë¡  ì ìš©"""
        
        # ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì‚¬ì‹¤ê³¼ ê·œì¹™ ì¶”ì¶œ
        facts = context.get("facts", [])
        rules = context.get("rules", [])
        
        # ì‚¬ì‹¤ ì¶”ê°€
        for fact in facts:
            self.logic.add_fact(fact)
            self._log_step("add_fact", {"fact": fact})
        
        # ê·œì¹™ ì¶”ê°€
        for rule in rules:
            self.logic.add_rule(
                rule.get("name", "rule"),
                rule.get("conditions", []),
                rule.get("conclusion", ""),
                rule.get("confidence", 1.0)
            )
        
        # ì¶”ë¡  ì‹¤í–‰
        new_facts = self.logic.forward_inference()
        consistency = self.logic.check_consistency()
        
        return {
            "method": "logical",
            "derived_facts": new_facts,
            "all_facts": list(self.logic.facts),
            "is_consistent": consistency["is_consistent"],
            "contradictions": consistency.get("contradictions", [])
        }
    
    def _apply_causal_reasoning(self, problem_type: Dict[str, Any], 
                               context: Dict[str, Any]) -> Dict[str, Any]:
        """ì¸ê³¼ ì¶”ë¡  ì ìš©"""
        
        # ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì¸ê³¼ê´€ê³„ ì •ë³´ ì¶”ì¶œ
        causal_links = context.get("causal_links", [])
        observations = context.get("observations", [])
        
        # ì¸ê³¼ ê·¸ë˜í”„ êµ¬ì¶•
        for link in causal_links:
            self.causal.add_causal_link(
                link["cause"],
                link["effect"],
                link.get("strength", 1.0)
            )
        
        # ê´€ì°° ì¶”ê°€
        for obs in observations:
            self.causal.observe(
                obs["variable"],
                obs["value"],
                obs.get("context", {})
            )
        
        # ì£¼ìš” ì¸ê³¼ê´€ê³„ ë¶„ì„
        target_cause = context.get("target_cause", "")
        target_effect = context.get("target_effect", "")
        
        if target_cause and target_effect:
            causal_analysis = self.causal.infer_causation(
                target_cause, target_effect
            )
        else:
            causal_analysis = {"message": "ì¸ê³¼ê´€ê³„ ëŒ€ìƒ ë¯¸ì§€ì •"}
        
        return {
            "method": "causal",
            "analysis": causal_analysis,
            "causal_graph_size": len(self.causal.causal_graph)
        }
    
    def _integrate_results(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """ì—¬ëŸ¬ ì¶”ë¡  ë°©ë²•ì˜ ê²°ê³¼ í†µí•©"""
        
        integrated = {
            "answer": "",
            "supporting_methods": [],
            "conflicts": []
        }
        
        # ê° ë°©ë²•ì˜ ì£¼ìš” ê²°ë¡  ì¶”ì¶œ
        conclusions = []
        
        if "bayesian" in results:
            bayesian_result = results["bayesian"]
            if bayesian_result["best_hypothesis"]:
                conclusions.append({
                    "method": "bayesian",
                    "conclusion": bayesian_result["best_hypothesis"],
                    "confidence": bayesian_result["probability"]
                })
        
        if "logical" in results:
            logical_result = results["logical"]
            if logical_result["derived_facts"]:
                conclusions.append({
                    "method": "logical",
                    "conclusion": ", ".join(logical_result["derived_facts"]),
                    "confidence": 1.0 if logical_result["is_consistent"] else 0.5
                })
        
        if "causal" in results:
            causal_result = results["causal"]
            if "analysis" in causal_result and causal_result["analysis"]:
                conclusions.append({
                    "method": "causal",
                    "conclusion": f"ì¸ê³¼ê´€ê³„ ì‹ ë¢°ë„: {causal_result['analysis'].get('causal_confidence', 0):.2f}",
                    "confidence": causal_result["analysis"].get("causal_confidence", 0)
                })
        
        # ê°€ì¥ ì‹ ë¢°ë„ ë†’ì€ ê²°ë¡  ì„ íƒ
        if conclusions:
            best_conclusion = max(conclusions, key=lambda x: x["confidence"])
            integrated["answer"] = best_conclusion["conclusion"]
            integrated["supporting_methods"] = [best_conclusion["method"]]
        else:
            integrated["answer"] = "ì¶©ë¶„í•œ ì •ë³´ê°€ ì—†ì–´ ê²°ë¡ ì„ ë‚´ë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
        
        return integrated
    
    def _assess_confidence(self, conclusion: Dict[str, Any], 
                         results: Dict[str, Any]) -> Dict[str, Any]:
        """ì¢…í•©ì  ì‹ ë¢°ë„ í‰ê°€"""
        
        # ê° ì¶”ë¡  ë°©ë²•ì˜ ì‹ ë¢°ë„ ìˆ˜ì§‘
        confidences = []
        
        if "bayesian" in results:
            confidences.append(results["bayesian"].get("probability", 0.5))
        
        if "logical" in results:
            is_consistent = results["logical"].get("is_consistent", True)
            confidences.append(1.0 if is_consistent else 0.3)
        
        if "causal" in results and "analysis" in results["causal"]:
            confidences.append(
                results["causal"]["analysis"].get("causal_confidence", 0.5)
            )
        
        # í‰ê·  ì‹ ë¢°ë„
        avg_confidence = np.mean(confidences) if confidences else 0.5
        
        # ì¦ê±° í’ˆì§ˆê³¼ ëª¨ë¸ ì •í™•ë„ ì¶”ì • (ê°„ë‹¨íˆ)
        evidence_quality = min(len(self.reasoning_trace) / 10, 1.0)
        model_accuracy = 0.7  # ê¸°ë³¸ê°’
        consistency_score = 1.0 if not any(
            r.get("contradictions") for r in results.values()
        ) else 0.5
        
        return self.uncertainty.calculate_confidence(
            evidence_quality,
            model_accuracy,
            consistency_score
        )
    
    def _identify_limitations(self, conclusion: Dict[str, Any], 
                            confidence: Dict[str, Any]) -> List[str]:
        """ì¶”ë¡ ì˜ í•œê³„ ì‹ë³„"""
        
        limitations = []
        
        if confidence["final_confidence"] < 0.7:
            limitations.append("ì‹ ë¢°ë„ê°€ ë‚®ì•„ ê²°ë¡ ì´ ë¶ˆí™•ì‹¤í•©ë‹ˆë‹¤")
        
        if not conclusion.get("supporting_methods"):
            limitations.append("ì–´ë–¤ ì¶”ë¡  ë°©ë²•ë„ ëª…í™•í•œ ë‹µì„ ì œê³µí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")
        
        if len(self.reasoning_trace) < 5:
            limitations.append("ì¶©ë¶„í•œ ì¶”ë¡  ë‹¨ê³„ë¥¼ ê±°ì¹˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")
        
        # í¸í–¥ ê²€ì‚¬ ê²°ê³¼ í™•ì¸
        bias_count = len(self.bias_detector.detected_biases)
        if bias_count > 0:
            limitations.append(f"{bias_count}ê°œì˜ ì¸ì§€ í¸í–¥ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤")
        
        if not limitations:
            limitations.append("ì¤‘ìš”í•œ í•œê³„ì ì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        return limitations
    
    def _log_step(self, step_type: str, data: Any) -> None:
        """ì¶”ë¡  ë‹¨ê³„ ê¸°ë¡"""
        self.reasoning_trace.append({
            "step": len(self.reasoning_trace) + 1,
            "type": step_type,
            "data": data,
            "timestamp": datetime.now().isoformat()
        })
    
    def explain_reasoning(self) -> str:
        """ì¶”ë¡  ê³¼ì • ì„¤ëª…"""
        
        explanation = f"\n=== {self.name}ì˜ ì¶”ë¡  ê³¼ì • ===\n"
        
        for step in self.reasoning_trace:
            explanation += f"\në‹¨ê³„ {step['step']} ({step['type']}):\n"
            
            if step['type'] == 'problem_analysis':
                explanation += f"  ë¬¸ì œ ìœ í˜•: {step['data']['type']}\n"
            elif step['type'] == 'bayesian_update':
                data = step['data']
                explanation += f"  ê°€ì„¤: {data['hypothesis']}\n"
                explanation += f"  ì‚¬ì „í™•ë¥ : {data['prior']:.3f} â†’ "
                explanation += f"ì‚¬í›„í™•ë¥ : {data['posterior']:.3f}\n"
            elif step['type'] == 'add_fact':
                explanation += f"  ì‚¬ì‹¤ ì¶”ê°€: {step['data']['fact']}\n"
            
        if self.conclusions:
            latest = self.conclusions[-1]
            explanation += f"\nìµœì¢… ê²°ë¡ : {latest['answer']}\n"
            explanation += f"ì‹ ë¢°ë„: {latest['confidence']:.2%}\n"
            
            if latest['limitations']:
                explanation += "\ní•œê³„ì :\n"
                for limitation in latest['limitations']:
                    explanation += f"  - {limitation}\n"
        
        return explanation


# ======================== ì‹¤ì œ ì‚¬ìš© ì˜ˆì œ ========================
def demonstrate_practical_reasoning():
    """ì‹¤ìš©ì  ì¶”ë¡  ì‹œìŠ¤í…œ ì‹œì—°"""
    
    print("ğŸš€ ì‹¤ìš©ì  ê³¼í•™ ì¶”ë¡  ì‹œìŠ¤í…œ v2.0")
    print("=" * 60)
    
    # ì¶”ë¡  ì‹œìŠ¤í…œ ìƒì„±
    reasoner = PracticalReasoner("Agnes_ì‹¤ìš©ì¶”ë¡ ê¸°")
    
    # ì˜ˆì œ 1: ì˜ë£Œ ì§„ë‹¨ ì¶”ë¡ 
    print("\nğŸ“‹ ì˜ˆì œ 1: ì˜ë£Œ ì§„ë‹¨")
    
    medical_context = {
        "hypotheses": ["ë…ê°", "ê°ê¸°", "ì½”ë¡œë‚˜19"],
        "evidence": [
            {
                "description": "ë°œì—´ 38.5ë„",
                "likelihood_if_true": {"ë…ê°": 0.9, "ê°ê¸°": 0.3, "ì½”ë¡œë‚˜19": 0.8},
                "likelihood_if_false": 0.1
            },
            {
                "description": "ê¸°ì¹¨",
                "likelihood_if_true": {"ë…ê°": 0.7, "ê°ê¸°": 0.8, "ì½”ë¡œë‚˜19": 0.9},
                "likelihood_if_false": 0.2
            }
        ],
        "facts": ["í™˜ìëŠ”_ë°œì—´ì¤‘", "í™˜ìëŠ”_ê¸°ì¹¨ì¤‘"],
        "rules": [
            {
                "name": "ë…ê°_ì§„ë‹¨_ê·œì¹™",
                "conditions": ["í™˜ìëŠ”_ë°œì—´ì¤‘", "í™˜ìëŠ”_ê¸°ì¹¨ì¤‘"],
                "conclusion": "ë…ê°_ê°€ëŠ¥ì„±_ë†’ìŒ"
            }
        ]
    }
    
    result1 = reasoner.reason(
        "í™˜ìì˜ ì¦ìƒì„ ë³´ê³  ê°€ì¥ ê°€ëŠ¥ì„± ë†’ì€ ì§ˆë³‘ì€?",
        medical_context
    )
    
    # ì˜ˆì œ 2: ì¸ê³¼ê´€ê³„ ì¶”ë¡ 
    print("\nğŸ“‹ ì˜ˆì œ 2: ì¸ê³¼ê´€ê³„ ë¶„ì„")
    
    causal_context = {
        "causal_links": [
            {"cause": "í¡ì—°", "effect": "íì•”", "strength": 0.7},
            {"cause": "ëŒ€ê¸°ì˜¤ì—¼", "effect": "íì•”", "strength": 0.4},
            {"cause": "ìœ ì „", "effect": "íì•”", "strength": 0.3}
        ],
        "observations": [
            {"variable": "í¡ì—°", "value": True},
            {"variable": "íì•”", "value": True}
        ],
        "target_cause": "í¡ì—°",
        "target_effect": "íì•”"
    }
    
    result2 = reasoner.reason(
        "í¡ì—°ì´ íì•”ì˜ ì›ì¸ì¼ ê°€ëŠ¥ì„±ì€?",
        causal_context
    )
    
    # ì¶”ë¡  ê³¼ì • ì„¤ëª…
    print("\n" + reasoner.explain_reasoning())
    
    return reasoner


if __name__ == "__main__":
    # ì‹œìŠ¤í…œ ì‹œì—°
    reasoner = demonstrate_practical_reasoning()
    
    print("\n" + "=" * 60)
    print("âœ… ì‹¤ìš©ì  ì¶”ë¡  ì‹œìŠ¤í…œì˜ íŠ¹ì§•:")
    print("1. ì‹¤ì œë¡œ ì‘ë™í•˜ëŠ” êµ¬í˜„")
    print("2. íˆ¬ëª…í•œ ì¶”ë¡  ê³¼ì •")
    print("3. ëª…í™•í•œ í•œê³„ ì¸ì •")
    print("4. í¸í–¥ ê°ì§€ ë° ë³´ì •")
    print("5. ì‹¤ìš©ì  ì‹ ë¢°ë„ í‰ê°€")
